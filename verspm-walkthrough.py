# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.4.2
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---

# %% [markdown]
# # VERSPM Model Interface

# %%
import emat
import os
import pandas as pd
import numpy as np
import gzip
from emat.util.show_dir import show_dir, show_file_contents

# %% [markdown]
# This notebook is meant to illustrate the use of TMIP-EMAT's
# with VisionEval's RSPM Model.  It provides an illustration of how to use 
# TMIP-EMAT and the interface to run the model.

# %% [markdown]
# In this example notebook, we will activate some logging features.  The 
# same logging utility is written directly into the EMAT and the
# `emat_verspm.py` module. This will give us a view of what's happening
# inside the code as it runs.

# %%
import logging
from emat.util.loggers import log_to_stderr
log = log_to_stderr(logging.INFO)

# %% [markdown]
# ## Connecting to the Model

# %% [markdown]
# The interface for this model is located in the `emat_verspm.py`
# module, which we will import into this notebook. 

# %%
import emat_verspm

# %% [markdown]
# Let's initialize a database file to store results.

# %%
database_path = os.path.expanduser("~/sandbox/ve-rspm-20200727.db")
initialize = not os.path.exists(database_path)
db = emat.SQLiteDB(database_path, initialize=initialize)

# %% [markdown]
# Within this module, you will find a definition for the 
# `VERSPModel` class.  We initialize an instance of the model interface object.

# %%
fx = emat_verspm.VERSPModel(db=db)

# %% [markdown]
# ## Single Run Operation for Development and Debugging

# %% [markdown]
# Before we take on the task of running this model in exploratory mode, we'll
# want to make sure that our interface code is working correctly. To check each
# of the components of the interface (setup, run, post-process, load-measures,
# and archive), we can run each individually in sequence, and inspect the results
# to make sure they are correct.

# %% [markdown]
# ### setup
#
# This method is the place where the core model *set up* takes place,
# including creating or modifying files as necessary to prepare
# for a core model run.  When running experiments, this method
# is called once for each core model experiment, where each experiment
# is defined by a set of particular values for both the exogenous
# uncertainties and the policy levers.  These values are passed to
# the experiment only here, and not in the `run` method itself.
# This facilitates debugging, as the `setup` method can be used 
# without the `run` method, as we do here. This allows us to manually
# inspect the prepared files and ensure they are correct before
# actually running a potentially expensive model.
#
# Each input exogenous uncertainty or policy lever can potentially
# be used to manipulate multiple different aspects of the underlying
# core model.  For example, a policy lever that includes a number of
# discrete future network "build" options might trigger the replacement
# of multiple related network definition files.  Or, a single uncertainty
# relating to the cost of fuel might scale both a parameter linked to
# the modeled per-mile cost of operating an automobile and the
# modeled total cost of fuel used by transit services.

# %% [markdown]
# In our RSPM module's `setup`, parameters that are omitted are set at their
# deafult values, but we can give a subset of parameters with non-default values
# if we like.

# %%
params = {
    'ValueOfTime': 13,
    'Income': 46300,
    'Transit': 1.34,
    'ElectricCost': 0.14,
    'FuelCost': 4.25,
} 

fx.setup(params)

# %% [markdown]
# ### run

# %% [markdown]
# The `run` method is the place where the core model run takes place.
# Note that this method takes no arguments; all the input
# exogenous uncertainties and policy levers are delivered to the
# core model in the `setup` method, which will be executed prior
# to calling this method. This facilitates debugging, as the `setup`
# method can be used without the `run` method as we did above, allowing
# us to manually inspect the prepared files and ensure they
# are correct before actually running a potentially expensive model.

# %%
fx.run()

# %% [markdown]
# The `VERSPModel` class includes a custom `last_run_logs` method,
# which displays both the "stdout" and "stderr" logs generated by the 
# model executable during the most recent call to the `run` method.
# We can use this method for debugging purposes, to identify why the 
# core model crashes (if it does crash).  In this first test it did not
# crash, and the logs look good.

# %%
show_dir(os.path.join(fx.master_directory.name, 'VERSPM', 'output'))

# %%
fx.last_run_logs()

# %%
os.path.join(fx.master_directory.name, 'VERSPM', 'output')

# %% [markdown]
# ### post-process
#
# There is a `post_process` step that is separate from the `run` step.
#
# For VERSPM, the post-processing replicates the calculations needed to
# create the same summary performance measures as the `R` version of
# VisionEval does when run with scenarios.

# %%
fx.post_process()

# %% [markdown]
# ### load-measures
#
# The `load_measures` method is the place to actually reach into
# files in the core model's run results and extract performance
# measures, returning a dictionary of key-value pairs for the 
# various performance measures. It takes an optional list giving a 
# subset of performance measures to load, and like the `post_process` 
# method also can be pointed at an archive location instead of loading 
# measures from the local working directory (which is the default).
# The `load_measures` method should not do any post-processing
# of results (i.e. it should read from but not write to the model
# outputs directory).

# %%
fx.load_measures()

# %% [markdown]
# You may note that the implementation of `RoadTestFileModel` in the `core_files_demo` module
# does not actually include a `load_measures` method itself, but instead inherits this method
# from the `FilesCoreModel` superclass. The instructions on how to actually find the relevant
# performance measures for this file are instead loaded into table parsers, which are defined
# in the `RoadTestFileModel.__init__` constructor.  There are [details and illustrations
# of how to write and use parsers in the file parsing examples page of the TMIP-EMAT documentation.](https://tmip-emat.github.io/source/emat.models/table_parse_example.html)

# %% [markdown]
# ### archive
#
# The `archive` method copies the relevant model output files to an archive location for 
# longer term storage.  The particular archive location is based on the experiment id
# for a particular experiment, and can be customized if desired by overloading the 
# `get_experiment_archive_path` method.  This customization is not done in this demo,
# so the default location is used.

# %%
fx.get_experiment_archive_path(parameters=params)

# %% [markdown]
# Actually running the `archive` method should copy any relevant output files
# from the `model_path` of the current active model into a subdirectory of `archive_path`.

# %%
fx.archive(params)

# %%
show_dir(fx.local_directory)

# %%
STOP

# %% [markdown]
# It is permissible, but not required, to simply copy the entire contents of the 
# former to the latter, as is done in this example. However, if the current active model
# directory has a lot of boilerplate files that don't change with the inputs, or
# if it becomes full of intermediate or temporary files that definitely will never
# be used to compute performance measures, it can be advisable to selectively copy
# only relevant files. In that case, those files and whatever related sub-directory
# tree structure exists in the current active model should be replicated within the
# experiments archive directory.

# %% [markdown]
# ## Normal Operation for Running Multiple Experiments

# %% [markdown]
# For this demo, we'll create a design of experiments with only 3 experiments.
# The `design_experiments` method of the `VERSPModel` object is not defined
# in the custom code written for this model, but rather is a generic
# function provide by the TMIP-EMAT main library.
# Real applications will typically use a larger number of experiments, but this small number
# is sufficient to demonstrate the operation of the tools.

# %%
design1 = fx.design_experiments(n_samples=3)
design1

# %% [markdown]
# The `run_experiments` command will automatically run the model once for each experiment in the named design.

# %%
fx.run_experiments(design1)

# %% [markdown]
# ## Multiprocessing for Running Multiple Experiments
#
# The examples above are all single-process demonstrations of using TMIP-EMAT to run core model
# VERSPM experiments. 
#
# This core model is single threaded, and such that you can run multiple independent instances of
# the model side-by-side on the same machine, so you can benefit from a multiprocessing 
# approach.  This can be accomplished by splitting a design of experiments over several
# processes that you start manually, or by using an automatic multiprocessing library such as 
# `dask.distributed`.

# %%
design3 = fx.design_experiments(design_name='lhs_a', random_seed=3, n_samples=6)
design3

# %% [markdown]
# The module is set up to facilitate distributed multiprocessing. During the `setup`
# step, the code detects if it is being run in a distributed "worker" environment instead of
# in a normal Python environment.  If the "worker" environment is detected, then a copy
# of the entire VERSPM model is made into the worker's local workspace, and the model
# is run there instead of in the master workspace.  This allows each worker to edit the files
# independently and simultaneously, without disturbing other parallel workers.
#
# To run the model with parallel subprocesses,
# we simply import the `get_client` function, and use that for the `evaluator` argument
# in the `run_experiments` method.

# %%
from emat.util.distributed import get_client # for multi-process operation

# %%
fx.run_experiments(design=design3, evaluator=get_client(n_workers=6))

# %%
